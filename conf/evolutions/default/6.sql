-- MySQL dump 10.13  Distrib 5.7.12, for osx10.9 (x86_64)
--
-- Host: 10.73.33.41    Database: playdb
-- ------------------------------------------------------
-- Server version	5.6.30-76.3

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `task_args`
--

DROP TABLE IF EXISTS `task_args`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `task_args` (
  `id` varchar(100) NOT NULL,
  `master` varchar(45) DEFAULT NULL,
  `executeClass` varchar(45) DEFAULT NULL,
  `numExecutors` varchar(45) DEFAULT NULL,
  `driverMemory` varchar(45) DEFAULT NULL,
  `executorMemory` varchar(45) DEFAULT NULL,
  `total_executor_cores` varchar(45) DEFAULT NULL,
  `jarLocation` varchar(500) DEFAULT NULL,
  `args1` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`id`)
)
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `task_args`
--

LOCK TABLES `task_args` WRITE;
/*!40000 ALTER TABLE `task_args` DISABLE KEYS */;
INSERT INTO `task_args` VALUES ('app-20160829173743-0025','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829173804-0026','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829173832-0027','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829173845-0028','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829174701-0029','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829175148-0030','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829181256-0031','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160829190518-0032','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160830110513-0000','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160830110605-0001','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160830111644-0002','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160830113932-0003','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('app-20160830114518-0005','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','100'),('app-20160830114548-0006','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','100'),('app-20160902143142-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('app-20160902180244-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905110614-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905112027-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905112717-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905113104-0003','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','4','/tmp/file/sparkStream.jar',''),('app-20160905113356-0004','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','4','/tmp/file/sparkStream.jar',''),('app-20160905120752-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905145226-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905145619-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905150324-0003','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905150833-0004','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160905151042-0005','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160907135634-0000','standalone','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','4','/tmp/file/sparkStream.jar','111133'),('app-20160907155656-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160907155814-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160907165415-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160907172220-0003','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('app-20160923182943-0007','standalone','BridgeUserDetector','2','1g','2g','10','/tmp/file/graph.jar','/user/king/graphx.data  /result'),('app-20160923183105-0008','standalone','BridgeUserDetector','2','1g','2g','10','/tmp/file/graph.jar','/user/king/graphx.data  /result'),('app-20161017171351-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161017172415-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161017173841-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161017175505-0003','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000000'),('app-20161017181857-0004','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000000'),('app-20161018173538-0005','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021144316-0022','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021145333-0023','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021145502-0024','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021145554-0025','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021145903-0026','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021150736-0027','standalone','org.apache.spark.examples.SparkPi','2','2g','2G','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161021152728-0033','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161021154141-0041','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161021174710-0042','standalone','org.apache.spark.examples.SparkPi','2','2G','2G','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021175213-0043','standalone','org.apache.spark.examples.SparkPi','2','2G','2G','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021175709-0044','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021175858-0045','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021181213-0000','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021181823-0001','standalone','org.apache.spark.examples.SparkPi','1','1g','1g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021182442-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','2','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161021182834-0007','standalone','org.apache.spark.examples.SparkPi','2','2g','2','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','11111'),('app-20161021192350-0008','standalone','org.apache.spark.examples.SparkPi','2','2g','2','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','11111'),('app-20161024112557-0009','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024143638-0003','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024154817-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024155208-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024155507-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024155919-0003','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161024161117-0004','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161107144820-0005','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161107145131-0006','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161110155841-0007','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar',''),('app-20161110155953-0008','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161110160212-0009','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161110160535-0010','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161110161633-0011','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161110162344-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161110170011-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161110170326-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('app-20161111155921-0012','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161111160527-0013','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161111160800-0014','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('app-20161223155534-0000','standalone','org.apache.spark.examples.SparkPi','2','1g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20161223155804-0001','standalone','org.apache.spark.examples.SparkPi','2','1g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20170209143254-0000','standalone','org.apache.spark.examples.SparkPi','2','1g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20170224173827-0000','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20170224174250-0001','standalone','org.apache.spark.examples.SparkPi','2','2g','2','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('app-20170224174652-0002','standalone','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('application_1472438450103_0002','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0003','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0004','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0005','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0006','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0007','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0008','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar','131 312 13 312 3 1 '),('application_1472438450103_0009','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','6','/tmp/file/sparkStream.jar',''),('application_1472438450103_0010','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','6','/tmp/file/sparkStream.jar',''),('application_1472438450103_0011','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','6','/tmp/file/sparkStream.jar',''),('application_1472438450103_0012','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','6','/tmp/file/sparkStream.jar',''),('application_1472538323529_0002','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','10000'),('application_1472611321353_0001','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0002','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0003','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0004','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0005','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0006','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0007','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0008','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','999999'),('application_1472611321353_0009','yarn-cluster','org.apache.spark.examples.SparkPi','2','2G','2G','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472611321353_0010','yarn-cluster','org.apache.spark.examples.SparkPi','2','2G','2G','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472611321353_0011','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472611321353_0012','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472611321353_0013','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472611321353_0014','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472611321353_0015','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472696197268_0001','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472696197268_0002','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472696197268_0003','yarn-cluster','mainq','2','2G','2G','6','/tmp/file/wc_2.10-1.0.jar','1000'),('application_1472696197268_0004','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0005','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0006','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0007','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0008','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0009','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0010','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472696197268_0011','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0001','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0002','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0003','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0004','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0005','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0006','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','1g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','200'),('application_1472781969555_0007','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472781969555_0008','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472781969555_0009','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472781969555_0010','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472781969555_0011','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','3g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1472781969555_0012','yarn-cluster','mainq','1','1g','1g','4','/tmp/file/wc_2.10-1.0.jar',''),('application_1472781969555_0013','yarn-cluster','mainq','1','1g','1g','4','/tmp/file/wc_2.10-1.0.jar',''),('application_1472781969555_0014','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','2g','3','/tmp/file/sparkStream.jar',''),('application_1473046703088_0001','yarn-cluster','org.apache.spark.examples.SparkPi','1','1g','1g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar',''),('application_1473046703088_0002','yarn-cluster','org.apache.spark.examples.SparkPi','1','1g','1g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar',''),('application_1473046703088_0003','yarn-cluster','org.apache.spark.examples.SparkPi','1','1g','1g','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1473046703088_0004','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','5','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1473228207769_0002','yarn-cluster','com.weibo.spark.stream.HDFSWordCount','2','2g','3g','5','/tmp/file/sparkStream.jar',''),('application_1473755209768_0002','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.4.jar','1000'),('application_1476358590704_0001','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','100000'),('application_1476931577277_0144','yarn-cluster','RecommendAtt','16','30g','90g','32','/tmp/file/spark2_2.11-1.0.jar',''),('application_1476931577277_0145','yarn-cluster','RecommendAtt','16','30g','90g','32','/tmp/file/spark2_2.11-1.0.jar',''),('application_1476931577277_0146','yarn-cluster','RecommendAtt','16','30g','90g','32','/tmp/file/spark2_2.11-1.0.jar',''),('application_1476931577277_0147','yarn-cluster','spark.graphx.RecoAttTest','10','10g','30g','2','/tmp/file/spark2_2.11-1.0.jar',''),('application_1477294157293_0013','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0014','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0015','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','3','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0016','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0017','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0021','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477294157293_0022','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2G','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('application_1477294157293_0023','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2G','6','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','1000'),('application_1477994556278_0004','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar',''),('application_1477994556278_0005','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000'),('application_1477994556278_0006','yarn-cluster','org.apache.spark.examples.SparkPi','2','2g','2g','4','/tmp/file/spark-examples-1.5.2-hadoop2.6.0.jar','10000');
/*!40000 ALTER TABLE `task_args` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-03-06 13:50:01
